{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import math\n",
    "# label\n",
    "# =========================\n",
    "def class_2_type(root):\n",
    "    label = \"\"\n",
    "    if \"正常\" in root:\n",
    "        label = \"0\"\n",
    "    else:\n",
    "        label = \"1\"\n",
    "    return label\n",
    "\n",
    "def class_3_type(root):\n",
    "    label = \"\"\n",
    "    if \"正常\" in root:\n",
    "        label = \"0\"\n",
    "    elif \"雙踝\" in root:\n",
    "        label = \"1\"\n",
    "    elif \"三踝\" in root:\n",
    "        label = \"2\"\n",
    "    return label\n",
    "# =========================\n",
    "\n",
    "def load_path(path, class_count):\n",
    "    dataset = []\n",
    "    class_type = ''\n",
    "    if class_count == 2:\n",
    "        class_type = class_2_type\n",
    "    elif class_count == 3:\n",
    "        class_type = class_3_type   \n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            label = class_type(root)\n",
    "            if label != \"\":\n",
    "                dataset.append(\n",
    "                                {   \n",
    "                                    'uuid': root.split(\"\\\\\")[-1],\n",
    "                                    'label': label,\n",
    "                                    'image_path': os.path.join(root, file)\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def multi_input_generator(front_gen, side_gen):\n",
    "    while True:\n",
    "        front_batch, y1 = next(front_gen)\n",
    "        side_batch, y2 = next(side_gen)\n",
    "        assert (y1 == y2).all(), \"Label mismatch!\"  # 確保標籤一致\n",
    "        yield ([front_batch, side_batch], y1)\n",
    "\n",
    "def create_front_extract():\n",
    "    pretrained_model_chosen = tf.keras.applications.resnet50.ResNet50\n",
    "    pretrained_model = pretrained_model_chosen(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg')\n",
    "    pretrained_model._name = 'AP_pretrain_resnet_model'\n",
    "    pretrained_model.trainable = False\n",
    "    return pretrained_model\n",
    "\n",
    "def create_side_extract():\n",
    "    pretrained_model_chosen = tf.keras.applications.efficientnet.EfficientNetB0\n",
    "    pretrained_model = pretrained_model_chosen(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg')\n",
    "    pretrained_model._name = 'Lateral_pretrain_efficientnet_model'\n",
    "    pretrained_model.trainable = False\n",
    "    return pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 參數設置\n",
    "image_dir = \"E:\\\\data_bone\\\\9-a+b_swift_cut_正確_V2\\\\front\"\n",
    "concat_type = \"concat2\"\n",
    "class_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data and  labels\n",
    "# =========================\n",
    "data = load_path(image_dir, class_count)\n",
    "labels = []\n",
    "filepaths = []\n",
    "for row in data:\n",
    "    labels.append(row['label'])\n",
    "    filepaths.append(row['image_path'])\n",
    "\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "images = pd.concat([filepaths, labels], axis=1)\n",
    "# =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split image\n",
    "# =========================\n",
    "train_df_front, test_df_front = train_test_split(images, train_size=0.8, shuffle=True, random_state=1, stratify=images['Label'])\n",
    "print(\"Training set label distribution:\\n\", train_df_front['Label'].value_counts(normalize=False))\n",
    "print(\"Test set label distribution:\\n\", test_df_front['Label'].value_counts(normalize=False))\n",
    "# =========================\n",
    "\n",
    "preprocessing_function_chosen_front = tf.keras.applications.resnet50.preprocess_input\n",
    "# preprocessing_function_chosen_side = tf.keras.applications.efficientnet.preprocess_input\n",
    "preprocessing_function_chosen_side = tf.keras.applications.resnet50.preprocess_input\n",
    "\n",
    "# front images\n",
    "# =========================\n",
    "train_generator_front = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=False,\n",
    "                                                                    preprocessing_function=preprocessing_function_chosen_front,\n",
    "                                                                    validation_split=0.2)\n",
    "test_generator_front = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_function_chosen_front)\n",
    "\n",
    "train_images_front = train_generator_front.flow_from_dataframe(\n",
    "    dataframe=train_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images_front = train_generator_front.flow_from_dataframe(\n",
    "    dataframe=train_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_images_front = test_generator_front.flow_from_dataframe(\n",
    "    dataframe=test_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "# =========================\n",
    "\n",
    "\n",
    "# side images\n",
    "# =========================\n",
    "train_df_side = train_df_front.copy()\n",
    "test_df_side = test_df_front.copy()\n",
    "train_df_side.loc[:, \"Filepath\"] = train_df_front[\"Filepath\"].str.replace(\"front\", \"side\")\n",
    "test_df_side.loc[:, \"Filepath\"] = test_df_side[\"Filepath\"].str.replace(\"front\", \"side\")\n",
    "\n",
    "train_generator_side = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=False,\n",
    "                                                                    preprocessing_function=preprocessing_function_chosen_side,\n",
    "                                                                    validation_split=0.2)\n",
    "test_generator_side = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_function_chosen_side)\n",
    "\n",
    "train_images_side = train_generator_side.flow_from_dataframe(\n",
    "    dataframe=train_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images_side = train_generator_side.flow_from_dataframe(\n",
    "    dataframe=train_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_images_side = test_generator_side.flow_from_dataframe(\n",
    "    dataframe=test_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "# =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df_side\n",
    "# test_df_front\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Lambda\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from deap import base, creator, tools, algorithms\n",
    "import math\n",
    "train_generator = multi_input_generator(train_images_front, train_images_side)\n",
    "val_generator = multi_input_generator(val_images_front, val_images_side)\n",
    "\n",
    "def create_model(gene_mask):\n",
    "    input_front = Input(shape=(224, 224, 3), name=\"AP(Mortise)\")\n",
    "    input_side = Input(shape=(224, 224, 3), name=\"Lateral\")\n",
    "\n",
    "    # 特徵提取\n",
    "    model_front = create_front_extract()\n",
    "    features_front = model_front(input_front)\n",
    "    features_front_x = Dense(128, activation='relu')(features_front)\n",
    "    features_front_x = Dense(50, activation='relu')(features_front_x)\n",
    "\n",
    "    model_side = create_side_extract()\n",
    "    features_side = model_side(input_side)\n",
    "    features_side_x = Dense(128, activation='relu')(features_side)\n",
    "    features_side_x = Dense(50, activation='relu')(features_side_x)\n",
    "\n",
    "    fused_features = Concatenate()([features_front_x, features_side_x])\n",
    "\n",
    "    # 使用基因選擇特徵\n",
    "    def apply_mask(tensor):\n",
    "        return tensor * tf.constant(gene_mask, dtype=tf.float32)  # 只保留基因為 1 的特徵\n",
    "\n",
    "    selected_features = Lambda(apply_mask)(fused_features)\n",
    "    \n",
    "    # selected_features = Dropout(0.1)(selected_features)\n",
    "    final_output = Dense(class_count, activation='sigmoid', name='output_layer')(selected_features)\n",
    "\n",
    "    model = Model(inputs=[input_front, input_side], outputs=final_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "## early stop \n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "score = []\n",
    "def evaluate_individual(individual):\n",
    "    global score\n",
    "    model = create_model(individual)  # 這裡的 individual 就是 100 維的 0/1 向量\n",
    "    batch_size = 64\n",
    "\n",
    "    ## early stop \n",
    "    history = model.fit(train_generator, validation_data=val_generator, callbacks=[early_stopping], epochs=30,\n",
    "                steps_per_epoch= math.ceil(train_images_front.samples / batch_size), \n",
    "                validation_steps= math.ceil(val_images_front.samples / batch_size),\n",
    "                verbose=1)\n",
    "    accuracy=max(history.history['val_accuracy'])\n",
    "\n",
    "    ## ==================\n",
    "    # history = model.fit(train_generator, validation_data=val_generator, epochs=30, \n",
    "    #                     steps_per_epoch=math.ceil(train_images_front.samples / batch_size), \n",
    "    #                     validation_steps=math.ceil(val_images_front.samples / batch_size), \n",
    "    #                     verbose=0)\n",
    "    # accuracy = history.history['val_accuracy'][-1]\n",
    "    ## ==================\n",
    "\n",
    "    # print(\"val_accuracy = \", accuracy)\n",
    "\n",
    "    ## print test results\n",
    "    # =========================\n",
    "    test_generator = multi_input_generator(test_images_front, test_images_side)\n",
    "\n",
    "    batch_size=32\n",
    "    pred = model.predict(test_generator,  steps=math.ceil(test_images_front.samples / batch_size))\n",
    "    predicted_labels = np.argmax(pred, axis=1)\n",
    "    acc = accuracy_score(test_images_front.labels, predicted_labels)\n",
    "    f1 = f1_score(test_images_front.labels, predicted_labels, average='macro')\n",
    "    # print(\"test_accuracy = \", acc)\n",
    "    # print(\"test_f1 = \", f1)\n",
    "    score.append([np.round(accuracy,2), np.round(acc,2), np.round(f1,2)])\n",
    "    print(\"-\")\n",
    "    # =========================\n",
    "    return accuracy,  # 必須返回元組\n",
    "\n",
    "tmp_best = []\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # 這表示我們是要最大化目標 (比如最大化準確度)\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "# 生成長度為 100 的 0/1 基因序列\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=100)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)  # 交叉\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)  # 突變，每個基因有 10% 機率翻轉\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # 選擇\n",
    "toolbox.register(\"evaluate\", evaluate_individual)  # 評估\n",
    "\n",
    "population_size = 50\n",
    "num_generations = 10\n",
    "\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "for generation in range(num_generations):\n",
    "    print(generation)\n",
    "    offspring = toolbox.select(population, len(population))\n",
    "    offspring = list(map(toolbox.clone, offspring))\n",
    "\n",
    "    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "        if random.random() < 0.7:\n",
    "            toolbox.mate(child1, child2)\n",
    "            del child1.fitness.values\n",
    "            del child2.fitness.values\n",
    "\n",
    "    for mutant in offspring:\n",
    "        if random.random() < 0.2:\n",
    "            toolbox.mutate(mutant)\n",
    "            del mutant.fitness.values\n",
    "\n",
    "    invalid_individuals = [ind for ind in offspring if not ind.fitness.valid]\n",
    "    fitnesses = list(map(toolbox.evaluate, invalid_individuals))\n",
    "\n",
    "    for ind, fit in zip(invalid_individuals, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    population[:] = offspring\n",
    "    best_individual = tools.selBest(population, 1)[0]\n",
    "    tmp_best.append(best_individual)\n",
    "    # print(evaluate_individual(best_individual)) \n",
    "    print(score[-1])\n",
    "    print(f\"目前最佳特徵選擇: {best_individual}\")\n",
    "    print(\"===========================================================================\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_index = next(i for i, ind in enumerate(population) if ind == best_individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_individuals = tools.selBest(population, 5)  # 選擇前 5 個最好的個體\n",
    "for i, individual in enumerate(best_individuals):\n",
    "    print(f\"Evaluating Feature Subset {i+1}: {individual}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用最佳特徵選擇重新創建模型\n",
    "# best_model = create_model(best_individual)\n",
    "\n",
    "# # 訓練最終模型\n",
    "# batch_size = 64\n",
    "# history = best_model.fit(train_generator, validation_data=val_generator, epochs=5, \n",
    "#                          steps_per_epoch=math.ceil(train_images_front.samples / batch_size), \n",
    "#                          validation_steps=math.ceil(val_images_front.samples / batch_size), \n",
    "#                          verbose=0)\n",
    "\n",
    "# # 存儲最佳模型\n",
    "# best_model.save(\"best_feature_selection_model.h5\")\n",
    "# print(\"最佳模型已保存為 best_feature_selection_model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bone_20240719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
