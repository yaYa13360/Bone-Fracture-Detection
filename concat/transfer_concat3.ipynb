{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import math\n",
    "# label\n",
    "# =========================\n",
    "def class_2_type(root):\n",
    "    label = \"\"\n",
    "    if \"正常\" in root:\n",
    "        label = \"0\"\n",
    "    else:\n",
    "        label = \"1\"\n",
    "    return label\n",
    "\n",
    "def class_3_type(root):\n",
    "    label = \"\"\n",
    "    if \"正常\" in root:\n",
    "        label = \"0\"\n",
    "    elif \"雙踝\" in root:\n",
    "        label = \"1\"\n",
    "    elif \"三踝\" in root:\n",
    "        label = \"2\"\n",
    "    return label\n",
    "# =========================\n",
    "\n",
    "def load_path(path, class_count):\n",
    "    dataset = []\n",
    "    class_type = ''\n",
    "    if class_count == 2:\n",
    "        class_type = class_2_type\n",
    "    elif class_count == 3:\n",
    "        class_type = class_3_type   \n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            label = class_type(root)\n",
    "            if label != \"\":\n",
    "                dataset.append(\n",
    "                                {   \n",
    "                                    'uuid': root.split(\"\\\\\")[-1],\n",
    "                                    'label': label,\n",
    "                                    'image_path': os.path.join(root, file)\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 參數設置\n",
    "image_dir = \"E:\\\\data_bone\\\\9-a+b_swift_cut_正確_V2\\\\front\"\n",
    "concat_type = \"concat3\"\n",
    "class_count = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data and  labels\n",
    "# =========================\n",
    "data = load_path(image_dir, class_count)\n",
    "labels = []\n",
    "filepaths = []\n",
    "for row in data:\n",
    "    labels.append(row['label'])\n",
    "    filepaths.append(row['image_path'])\n",
    "\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "images = pd.concat([filepaths, labels], axis=1)\n",
    "# =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set label distribution:\n",
      " 0    128\n",
      "2     95\n",
      "1     93\n",
      "Name: Label, dtype: int64\n",
      "Test set label distribution:\n",
      " 0    32\n",
      "1    24\n",
      "2    24\n",
      "Name: Label, dtype: int64\n",
      "Found 253 validated image filenames belonging to 3 classes.\n",
      "Found 63 validated image filenames belonging to 3 classes.\n",
      "Found 80 validated image filenames belonging to 3 classes.\n",
      "Found 253 validated image filenames belonging to 3 classes.\n",
      "Found 63 validated image filenames belonging to 3 classes.\n",
      "Found 80 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "## split image\n",
    "# =========================\n",
    "train_df_front, test_df_front = train_test_split(images, train_size=0.8, shuffle=True, random_state=1, stratify=images['Label'])\n",
    "print(\"Training set label distribution:\\n\", train_df_front['Label'].value_counts(normalize=False))\n",
    "print(\"Test set label distribution:\\n\", test_df_front['Label'].value_counts(normalize=False))\n",
    "# =========================\n",
    "\n",
    "preprocessing_function_chosen_front = tf.keras.applications.resnet50.preprocess_input\n",
    "preprocessing_function_chosen_side = tf.keras.applications.efficientnet.preprocess_input\n",
    "\n",
    "# front images\n",
    "# =========================\n",
    "train_generator_front = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=False,\n",
    "                                                                    preprocessing_function=preprocessing_function_chosen_front,\n",
    "                                                                    validation_split=0.2)\n",
    "test_generator_front = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_function_chosen_front)\n",
    "\n",
    "train_images_front = train_generator_front.flow_from_dataframe(\n",
    "    dataframe=train_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images_front = train_generator_front.flow_from_dataframe(\n",
    "    dataframe=train_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_images_front = test_generator_front.flow_from_dataframe(\n",
    "    dataframe=test_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "# =========================\n",
    "\n",
    "\n",
    "# side images\n",
    "# =========================\n",
    "train_df_side = train_df_front.copy()\n",
    "test_df_side = test_df_front.copy()\n",
    "train_df_side.loc[:, \"Filepath\"] = train_df_front[\"Filepath\"].str.replace(\"front\", \"side\")\n",
    "test_df_side.loc[:, \"Filepath\"] = test_df_side[\"Filepath\"].str.replace(\"front\", \"side\")\n",
    "\n",
    "train_generator_side = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=False,\n",
    "                                                                    preprocessing_function=preprocessing_function_chosen_side,\n",
    "                                                                    validation_split=0.2)\n",
    "test_generator_side = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_function_chosen_side)\n",
    "\n",
    "train_images_side = train_generator_side.flow_from_dataframe(\n",
    "    dataframe=train_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images_side = train_generator_side.flow_from_dataframe(\n",
    "    dataframe=train_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_images_side = test_generator_side.flow_from_dataframe(\n",
    "    dataframe=test_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "# =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df_side\n",
    "# test_df_front\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "AP(Mortise) (InputLayer)        [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Lateral (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Functional)           (None, 2048)         23587712    AP(Mortise)[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "efficientnetb0 (Functional)     (None, 1280)         4049571     Lateral[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "AP_dense_2048 (Dense)           (None, 2048)         4196352     resnet50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Lateral_dense_1280 (Dense)      (None, 1280)         1639680     efficientnetb0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "AP_dense_128 (Dense)            (None, 128)          262272      AP_dense_2048[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Lateral_dense_128 (Dense)       (None, 128)          163968      Lateral_dense_1280[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "AP_dense_50 (Dense)             (None, 50)           6450        AP_dense_128[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Lateral_dense_50 (Dense)        (None, 50)           6450        Lateral_dense_128[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "feature_fusion (Concatenate)    (None, 100)          0           AP_dense_50[0][0]                \n",
      "                                                                 Lateral_dense_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           feature_fusion[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 3)            303         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 33,912,758\n",
      "Trainable params: 6,275,475\n",
      "Non-trainable params: 27,637,283\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "\n",
    "\n",
    "def create_front_extract():\n",
    "    pretrained_model_chosen = tf.keras.applications.resnet50.ResNet50\n",
    "    pretrained_model = pretrained_model_chosen(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg')\n",
    "    pretrained_model.trainable = False\n",
    "    return pretrained_model\n",
    "\n",
    "def create_side_extract():\n",
    "    pretrained_model_chosen = tf.keras.applications.efficientnet.EfficientNetB0\n",
    "    pretrained_model = pretrained_model_chosen(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg')\n",
    "    pretrained_model.trainable = False\n",
    "    return pretrained_model\n",
    "\n",
    "# input \n",
    "input_front = Input(shape=(224, 224, 3), name=\"AP(Mortise)\")\n",
    "input_side = Input(shape=(224, 224, 3), name=\"Lateral\")\n",
    "\n",
    "# model\n",
    "model_front = create_front_extract()\n",
    "model_side = create_side_extract()\n",
    "\n",
    "features_front = model_front(input_front)\n",
    "features_side = model_side(input_side)\n",
    "\n",
    "features_front_x = tf.keras.layers.Dense(2048, activation='relu', name='AP_dense_2048')(features_front)\n",
    "features_front_x = tf.keras.layers.Dense(128, activation='relu', name='AP_dense_128')(features_front_x)\n",
    "features_front_x = tf.keras.layers.Dense(50, activation='relu', name='AP_dense_50')(features_front_x)\n",
    "\n",
    "features_side_x = tf.keras.layers.Dense(1280, activation='relu', name='Lateral_dense_1280')(features_side)\n",
    "features_side_x = tf.keras.layers.Dense(128, activation='relu', name='Lateral_dense_128')(features_side_x)\n",
    "features_side_x = tf.keras.layers.Dense(50, activation='relu', name='Lateral_dense_50')(features_side_x)\n",
    "\n",
    "fused_features = Concatenate(name=\"feature_fusion\")([features_front_x, features_side_x])\n",
    "# fused_features = tf.keras.layers.Dense(50, activation='relu', name='fusion_dense_50')(fused_features)\n",
    "fused_features = Dropout(0.2)(fused_features)\n",
    "\n",
    "final_output = Dense(class_count, activation='sigmoid', name='output_layer')(fused_features)\n",
    "multi_view_model = None\n",
    "multi_view_model = Model(\n",
    "    inputs=[input_front, input_side],\n",
    "    outputs=final_output\n",
    ")\n",
    "multi_view_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "multi_view_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_input_generator(front_gen, side_gen):\n",
    "    while True:\n",
    "        front_batch, y1 = next(front_gen)\n",
    "        side_batch, y2 = next(side_gen)\n",
    "        assert (y1 == y2).all(), \"Label mismatch!\"  # 確保標籤一致\n",
    "        yield ([front_batch, side_batch], y1)\n",
    "\n",
    "train_generator = multi_input_generator(train_images_front, train_images_side)\n",
    "val_generator = multi_input_generator(val_images_front, val_images_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Training _concat3-------\n",
      "Epoch 1/30\n",
      "4/4 [==============================] - 6s 857ms/step - loss: 1.2294 - accuracy: 0.3241 - val_loss: 0.9092 - val_accuracy: 0.6032\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 2s 595ms/step - loss: 0.8842 - accuracy: 0.6206 - val_loss: 0.8134 - val_accuracy: 0.6984\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 2s 584ms/step - loss: 0.7452 - accuracy: 0.7312 - val_loss: 0.7016 - val_accuracy: 0.7302\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 2s 570ms/step - loss: 0.6369 - accuracy: 0.7470 - val_loss: 0.6362 - val_accuracy: 0.7302\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 2s 573ms/step - loss: 0.5275 - accuracy: 0.8221 - val_loss: 0.6029 - val_accuracy: 0.7143\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 2s 572ms/step - loss: 0.4896 - accuracy: 0.8182 - val_loss: 0.5765 - val_accuracy: 0.7143\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.4171 - accuracy: 0.8538 - val_loss: 0.5608 - val_accuracy: 0.7143\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 2s 571ms/step - loss: 0.4220 - accuracy: 0.8261 - val_loss: 0.5530 - val_accuracy: 0.7460\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 2s 569ms/step - loss: 0.3697 - accuracy: 0.8814 - val_loss: 0.5427 - val_accuracy: 0.7302\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 2s 569ms/step - loss: 0.3158 - accuracy: 0.8933 - val_loss: 0.5304 - val_accuracy: 0.7460\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 2s 571ms/step - loss: 0.3015 - accuracy: 0.9051 - val_loss: 0.5233 - val_accuracy: 0.7619\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 2s 571ms/step - loss: 0.2688 - accuracy: 0.9447 - val_loss: 0.5165 - val_accuracy: 0.7778\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 2s 565ms/step - loss: 0.2489 - accuracy: 0.9368 - val_loss: 0.5125 - val_accuracy: 0.7937\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - 2s 565ms/step - loss: 0.2370 - accuracy: 0.9526 - val_loss: 0.5128 - val_accuracy: 0.7778\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - 2s 566ms/step - loss: 0.2040 - accuracy: 0.9605 - val_loss: 0.5042 - val_accuracy: 0.7937\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - 2s 566ms/step - loss: 0.1988 - accuracy: 0.9723 - val_loss: 0.5042 - val_accuracy: 0.8095\n",
      "Epoch 17/30\n",
      "4/4 [==============================] - 2s 566ms/step - loss: 0.1640 - accuracy: 0.9842 - val_loss: 0.5029 - val_accuracy: 0.8095\n",
      "Epoch 18/30\n",
      "4/4 [==============================] - 2s 573ms/step - loss: 0.1596 - accuracy: 0.9644 - val_loss: 0.5004 - val_accuracy: 0.7937\n",
      "Epoch 19/30\n",
      "4/4 [==============================] - 2s 569ms/step - loss: 0.1592 - accuracy: 0.9684 - val_loss: 0.5041 - val_accuracy: 0.7778\n",
      "Epoch 20/30\n",
      "4/4 [==============================] - 2s 567ms/step - loss: 0.1325 - accuracy: 0.9723 - val_loss: 0.5151 - val_accuracy: 0.8095\n",
      "Epoch 21/30\n",
      "4/4 [==============================] - 2s 571ms/step - loss: 0.1139 - accuracy: 0.9960 - val_loss: 0.5107 - val_accuracy: 0.7937\n",
      "Epoch 22/30\n",
      "4/4 [==============================] - 2s 571ms/step - loss: 0.1074 - accuracy: 0.9960 - val_loss: 0.5120 - val_accuracy: 0.7937\n",
      "Epoch 23/30\n",
      "4/4 [==============================] - 2s 572ms/step - loss: 0.1051 - accuracy: 0.9881 - val_loss: 0.5150 - val_accuracy: 0.7937\n",
      "Epoch 24/30\n",
      "4/4 [==============================] - 2s 573ms/step - loss: 0.0978 - accuracy: 0.9881 - val_loss: 0.5134 - val_accuracy: 0.8095\n",
      "Epoch 25/30\n",
      "4/4 [==============================] - 2s 575ms/step - loss: 0.0891 - accuracy: 0.9960 - val_loss: 0.5035 - val_accuracy: 0.7937\n",
      "Epoch 26/30\n",
      "4/4 [==============================] - 2s 574ms/step - loss: 0.0782 - accuracy: 0.9960 - val_loss: 0.5185 - val_accuracy: 0.7937\n",
      "Epoch 27/30\n",
      "4/4 [==============================] - 2s 572ms/step - loss: 0.0848 - accuracy: 0.9881 - val_loss: 0.5280 - val_accuracy: 0.7937\n",
      "Epoch 28/30\n",
      "4/4 [==============================] - 2s 570ms/step - loss: 0.0634 - accuracy: 1.0000 - val_loss: 0.5202 - val_accuracy: 0.8095\n",
      "Epoch 29/30\n",
      "4/4 [==============================] - 2s 576ms/step - loss: 0.0630 - accuracy: 0.9960 - val_loss: 0.5362 - val_accuracy: 0.7778\n",
      "Epoch 30/30\n",
      "4/4 [==============================] - 2s 568ms/step - loss: 0.0566 - accuracy: 0.9960 - val_loss: 0.5382 - val_accuracy: 0.7778\n"
     ]
    }
   ],
   "source": [
    "## compile and evaluate\n",
    "# =========================\n",
    "\n",
    "print(\"-------Training \" + \"_\" + concat_type + \"-------\")\n",
    "batch_size = 64\n",
    "## early stop \n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "# multi_view_model.fit(train_generator, validation_data=val_generator, callbacks=[early_stopping], epochs=30,\n",
    "#                     steps_per_epoch= math.ceil(train_images_front.samples / batch_size), \n",
    "#                     validation_steps= math.ceil(val_images_front.samples / batch_size))\n",
    "## no early stop\n",
    "history = multi_view_model.fit(train_generator, validation_data=val_generator, epochs=30,\n",
    "                            steps_per_epoch= math.ceil(train_images_front.samples / batch_size), \n",
    "                            validation_steps= math.ceil(val_images_front.samples / batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\flwr\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## save model to this path\n",
    "# =========================\n",
    "multi_view_model.save(\"./weights/\"+concat_type+\"_\" + \"_frac.h5\")\n",
    "# =========================\n",
    "\n",
    "\n",
    "## print results\n",
    "# =========================\n",
    "test_generator = multi_input_generator(test_images_front, test_images_side)\n",
    "\n",
    "batch_size=32\n",
    "pred = multi_view_model.predict(test_generator,  steps=math.ceil(test_images_front.samples / batch_size))\n",
    "predicted_labels = np.argmax(pred, axis=1)\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "# create plots for accuracy and save it\n",
    "# =========================\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "figAcc = plt.gcf()\n",
    "my_file = os.path.join(\"./plots/\"+concat_type+\"_Accuracy.jpeg\")\n",
    "figAcc.savefig(my_file)\n",
    "plt.clf()\n",
    "# =========================\n",
    "\n",
    "\n",
    "## create plots for loss and save it\n",
    "# =========================\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "figAcc = plt.gcf()\n",
    "my_file = os.path.join(\"./plots/\"+concat_type+\"_\"+\"_Loss.jpeg\")\n",
    "figAcc.savefig(my_file)\n",
    "plt.clf()\n",
    "# =========================\n",
    "\n",
    "\n",
    "## plot confusion matrix\n",
    "# =========================\n",
    "if class_count == 2:\n",
    "    display_labels = [0, 1]\n",
    "elif class_count == 3:\n",
    "    display_labels = [0, 1, 2]\n",
    "elif class_count == 4:\n",
    "    display_labels = [0, 1, 2, 3]\n",
    "\n",
    "\n",
    "cm = confusion_matrix(test_images_front.labels, predicted_labels)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = display_labels)\n",
    "cm_display.plot()\n",
    "plt.title('Confusion Matrix')\n",
    "figAcc = plt.gcf()\n",
    "my_file = os.path.join(\"./plots/\"+concat_type+\"_\"+\"_Confusion Matrix.jpeg\")\n",
    "figAcc.savefig(my_file)\n",
    "plt.clf()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flwr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
