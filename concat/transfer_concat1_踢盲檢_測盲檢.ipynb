{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import math\n",
    "# label\n",
    "# =========================\n",
    "def class_2_type(root):\n",
    "    label = \"\"\n",
    "    if \"正常\" in root:\n",
    "        label = \"0\"\n",
    "    else:\n",
    "        label = \"1\"\n",
    "    return label\n",
    "\n",
    "def class_3_type(root):\n",
    "    label = \"\"\n",
    "    if \"正常\" in root:\n",
    "        label = \"0\"\n",
    "    elif \"雙踝\" in root:\n",
    "        label = \"1\"\n",
    "    elif \"三踝\" in root:\n",
    "        label = \"2\"\n",
    "    return label\n",
    "# =========================\n",
    "\n",
    "def load_path(path, class_count):\n",
    "    dataset = []\n",
    "    class_type = ''\n",
    "    if class_count == 2:\n",
    "        class_type = class_2_type\n",
    "    elif class_count == 3:\n",
    "        class_type = class_3_type   \n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            label = class_type(root)\n",
    "            if label != \"\":\n",
    "                dataset.append(\n",
    "                                {   \n",
    "                                    'uuid': root.split(\"\\\\\")[-1],\n",
    "                                    'label': label,\n",
    "                                    'image_path': os.path.join(root, file)\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "def plot_img(image_path):\n",
    "    print(image_path)\n",
    "    # image = cv2.imread(image_path)\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # OpenCV 讀取的圖像是 BGR 需要轉為 RGB\n",
    "    image1 = cv2.imdecode(np.fromfile(image_path, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "    image2 = cv2.imdecode(np.fromfile(image_path.replace(\"front\", \"side\"), dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "\n",
    "\n",
    "    # **顯示圖片**\n",
    "    plt.figure(figsize=(8, 5))  # 設定圖片大小\n",
    "\n",
    "    # 顯示第一張圖（front）\n",
    "    plt.subplot(1, 2, 1)  # (行數, 列數, 當前索引)\n",
    "    plt.imshow(image1)\n",
    "    plt.title(\"AP(Mortise) View\")\n",
    "    plt.axis(\"off\")  # 隱藏座標軸\n",
    "\n",
    "    # 顯示第二張圖（side）\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image2)\n",
    "    plt.title(\"Lateral View\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(image_path.split(\"\\\\\")[-1])\n",
    "\n",
    "    plt.show()  # 顯示圖片\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 參數設置\n",
    "image_dir = \"E:\\\\data_bone\\\\11-a+b_swift_cut_正確_V2_踢盲檢\\\\front\"\n",
    "image_dir2 = \"E:\\\\data_bone\\\\11-只有盲檢\\\\front\"\n",
    "concat_type = \"concat1_踢盲檢_測盲測\"\n",
    "class_count = 3\n",
    "save_cam_path = \"D://reaserch//Bone-Fracture-Detection//concat//2-concat1//踢盲檢_測盲測_cam//\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data and  labels\n",
    "# =========================\n",
    "data = load_path(image_dir, class_count)\n",
    "labels = []\n",
    "filepaths = []\n",
    "for row in data:\n",
    "    labels.append(row['label'])\n",
    "    filepaths.append(row['image_path'])\n",
    "\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "images = pd.concat([filepaths, labels], axis=1)\n",
    "# =========================\n",
    "\n",
    "## load data and  labels\n",
    "# =========================\n",
    "data2 = load_path(image_dir2, class_count)\n",
    "labels = []\n",
    "filepaths = []\n",
    "for row in data2:\n",
    "    labels.append(row['label'])\n",
    "    filepaths.append(row['image_path'])\n",
    "\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "images2 = pd.concat([filepaths, labels], axis=1)\n",
    "# =========================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set label distribution:\n",
      " 0    126\n",
      "2     87\n",
      "1     84\n",
      "Name: Label, dtype: int64\n",
      "Test set label distribution:\n",
      " 2    35\n",
      "0    34\n",
      "1    31\n",
      "Name: Label, dtype: int64\n",
      "Found 238 validated image filenames belonging to 3 classes.\n",
      "Found 59 validated image filenames belonging to 3 classes.\n",
      "Found 100 validated image filenames belonging to 3 classes.\n",
      "Found 238 validated image filenames belonging to 3 classes.\n",
      "Found 59 validated image filenames belonging to 3 classes.\n",
      "Found 100 validated image filenames belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "## split image\n",
    "# =========================\n",
    "# train_df_front, test_df_front = train_test_split(images, train_size=0.8, shuffle=True, random_state=1, stratify=images['Label'])\n",
    "train_df_front = images.sample(frac=1, random_state=1)\n",
    "test_df_front = images2\n",
    "print(\"Training set label distribution:\\n\", train_df_front['Label'].value_counts(normalize=False))\n",
    "print(\"Test set label distribution:\\n\", test_df_front['Label'].value_counts(normalize=False))\n",
    "# =========================\n",
    "\n",
    "preprocessing_function_chosen_front = tf.keras.applications.resnet50.preprocess_input\n",
    "preprocessing_function_chosen_side = tf.keras.applications.efficientnet.preprocess_input\n",
    "\n",
    "# front images\n",
    "# =========================\n",
    "train_generator_front = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=False,\n",
    "                                                                    preprocessing_function=preprocessing_function_chosen_front,\n",
    "                                                                    validation_split=0.2)\n",
    "test_generator_front = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_function_chosen_front)\n",
    "\n",
    "train_images_front = train_generator_front.flow_from_dataframe(\n",
    "    dataframe=train_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images_front = train_generator_front.flow_from_dataframe(\n",
    "    dataframe=train_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    seed=42,\n",
    "    shuffle=False,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_images_front = test_generator_front.flow_from_dataframe(\n",
    "    dataframe=test_df_front,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "# =========================\n",
    "\n",
    "\n",
    "# side images\n",
    "# =========================\n",
    "train_df_side = train_df_front.copy()\n",
    "test_df_side = test_df_front.copy()\n",
    "train_df_side.loc[:, \"Filepath\"] = train_df_front[\"Filepath\"].str.replace(\"front\", \"side\")\n",
    "test_df_side.loc[:, \"Filepath\"] = test_df_side[\"Filepath\"].str.replace(\"front\", \"side\")\n",
    "\n",
    "train_generator_side = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=False,\n",
    "                                                                    preprocessing_function=preprocessing_function_chosen_side,\n",
    "                                                                    validation_split=0.2)\n",
    "test_generator_side = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_function_chosen_side)\n",
    "\n",
    "train_images_side = train_generator_side.flow_from_dataframe(\n",
    "    dataframe=train_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_images_side = train_generator_side.flow_from_dataframe(\n",
    "    dataframe=train_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    seed=42,\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_images_side = test_generator_side.flow_from_dataframe(\n",
    "    dataframe=test_df_side,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(224, 224),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "# =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_821\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "AP(Mortise) (InputLayer)        [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Lateral (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Functional)           (None, 2048)         23587712    AP(Mortise)[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "efficientnetb0 (Functional)     (None, 1280)         4049571     Lateral[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "AP_dense_128 (Dense)            (None, 128)          262272      resnet50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Lateral_dense_128 (Dense)       (None, 128)          163968      efficientnetb0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "AP_dense_50 (Dense)             (None, 50)           6450        AP_dense_128[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Lateral_dense_50 (Dense)        (None, 50)           6450        Lateral_dense_128[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "feature_fusion (Concatenate)    (None, 100)          0           AP_dense_50[0][0]                \n",
      "                                                                 Lateral_dense_50[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 100)          0           feature_fusion[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output_layer (Dense)            (None, 3)            303         dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,076,726\n",
      "Trainable params: 439,443\n",
      "Non-trainable params: 27,637,283\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, Dropout\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "\n",
    "\n",
    "def create_front_extract():\n",
    "    pretrained_model_chosen = tf.keras.applications.resnet50.ResNet50\n",
    "    pretrained_model = pretrained_model_chosen(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg')\n",
    "    pretrained_model.trainable = False\n",
    "    return pretrained_model\n",
    "\n",
    "def create_side_extract():\n",
    "    pretrained_model_chosen = tf.keras.applications.efficientnet.EfficientNetB0\n",
    "    pretrained_model = pretrained_model_chosen(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='avg')\n",
    "    pretrained_model.trainable = False\n",
    "    return pretrained_model\n",
    "\n",
    "# input \n",
    "input_front = Input(shape=(224, 224, 3), name=\"AP(Mortise)\")\n",
    "input_side = Input(shape=(224, 224, 3), name=\"Lateral\")\n",
    "\n",
    "# model\n",
    "model_front = create_front_extract()\n",
    "model_side = create_side_extract()\n",
    "\n",
    "features_front = model_front(input_front)\n",
    "features_side = model_side(input_side)\n",
    "\n",
    "features_front_x = tf.keras.layers.Dense(128, activation='relu', name='AP_dense_128')(features_front)\n",
    "features_front_x = tf.keras.layers.Dense(50, activation='relu', name='AP_dense_50')(features_front_x)\n",
    "\n",
    "features_side_x = tf.keras.layers.Dense(128, activation='relu', name='Lateral_dense_128')(features_side)\n",
    "features_side_x = tf.keras.layers.Dense(50, activation='relu', name='Lateral_dense_50')(features_side_x)\n",
    "\n",
    "fused_features = Concatenate(name=\"feature_fusion\")([features_front_x, features_side_x])\n",
    "# fused_features = tf.keras.layers.Dense(50, activation='relu', name='fusion_dense_50')(fused_features)\n",
    "fused_features = Dropout(0.1)(fused_features)\n",
    "\n",
    "final_output = Dense(class_count, activation='sigmoid', name='output_layer')(fused_features)\n",
    "multi_view_model = None\n",
    "multi_view_model = Model(\n",
    "    inputs=[input_front, input_side],\n",
    "    outputs=final_output\n",
    ")\n",
    "multi_view_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "multi_view_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_input_generator(front_gen, side_gen):\n",
    "    while True:\n",
    "        front_batch, y1 = next(front_gen)\n",
    "        side_batch, y2 = next(side_gen)\n",
    "        assert (y1 == y2).all(), \"Label mismatch!\"  # 確保標籤一致\n",
    "        yield ([front_batch, side_batch], y1)\n",
    "\n",
    "train_generator = multi_input_generator(train_images_front, train_images_side)\n",
    "val_generator = multi_input_generator(val_images_front, val_images_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Training _concat1_踢盲檢_測盲測-------\n",
      "Epoch 1/30\n",
      "4/4 [==============================] - 6s 906ms/step - loss: 1.0916 - accuracy: 0.4454 - val_loss: 0.9702 - val_accuracy: 0.5085\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 2s 540ms/step - loss: 0.9667 - accuracy: 0.5840 - val_loss: 0.8941 - val_accuracy: 0.6441\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 2s 540ms/step - loss: 0.8789 - accuracy: 0.6849 - val_loss: 0.8282 - val_accuracy: 0.6610\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 2s 548ms/step - loss: 0.7848 - accuracy: 0.6975 - val_loss: 0.7623 - val_accuracy: 0.7119\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 2s 539ms/step - loss: 0.7278 - accuracy: 0.7101 - val_loss: 0.7080 - val_accuracy: 0.6949\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.6823 - accuracy: 0.7605 - val_loss: 0.6695 - val_accuracy: 0.6949\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 2s 540ms/step - loss: 0.6310 - accuracy: 0.7899 - val_loss: 0.6414 - val_accuracy: 0.7288\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 2s 544ms/step - loss: 0.5836 - accuracy: 0.8109 - val_loss: 0.6200 - val_accuracy: 0.7119\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.5897 - accuracy: 0.7857 - val_loss: 0.5971 - val_accuracy: 0.6949\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.5439 - accuracy: 0.8025 - val_loss: 0.5744 - val_accuracy: 0.7119\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 2s 537ms/step - loss: 0.5000 - accuracy: 0.8445 - val_loss: 0.5541 - val_accuracy: 0.7119\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 2s 536ms/step - loss: 0.4833 - accuracy: 0.8866 - val_loss: 0.5398 - val_accuracy: 0.7288\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.4529 - accuracy: 0.8319 - val_loss: 0.5280 - val_accuracy: 0.7288\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - 2s 533ms/step - loss: 0.4287 - accuracy: 0.8655 - val_loss: 0.5172 - val_accuracy: 0.7288\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - 2s 543ms/step - loss: 0.4234 - accuracy: 0.8824 - val_loss: 0.5040 - val_accuracy: 0.7458\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - 2s 539ms/step - loss: 0.3902 - accuracy: 0.8866 - val_loss: 0.5019 - val_accuracy: 0.7627\n",
      "Epoch 17/30\n",
      "4/4 [==============================] - 2s 587ms/step - loss: 0.3746 - accuracy: 0.9076 - val_loss: 0.5035 - val_accuracy: 0.7458\n",
      "Epoch 18/30\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.3712 - accuracy: 0.8866 - val_loss: 0.4960 - val_accuracy: 0.7627\n",
      "Epoch 19/30\n",
      "4/4 [==============================] - 2s 547ms/step - loss: 0.3506 - accuracy: 0.9076 - val_loss: 0.4803 - val_accuracy: 0.7627\n",
      "Epoch 20/30\n",
      "4/4 [==============================] - 2s 541ms/step - loss: 0.3345 - accuracy: 0.9202 - val_loss: 0.4818 - val_accuracy: 0.7797\n",
      "Epoch 21/30\n",
      "4/4 [==============================] - 2s 534ms/step - loss: 0.3312 - accuracy: 0.8908 - val_loss: 0.4821 - val_accuracy: 0.7797\n",
      "Epoch 22/30\n",
      "4/4 [==============================] - 2s 534ms/step - loss: 0.3102 - accuracy: 0.9160 - val_loss: 0.4711 - val_accuracy: 0.7797\n",
      "Epoch 23/30\n",
      "4/4 [==============================] - 2s 540ms/step - loss: 0.2923 - accuracy: 0.9538 - val_loss: 0.4690 - val_accuracy: 0.7458\n",
      "Epoch 24/30\n",
      "4/4 [==============================] - 2s 539ms/step - loss: 0.2885 - accuracy: 0.9412 - val_loss: 0.4801 - val_accuracy: 0.7627\n",
      "Epoch 25/30\n",
      "4/4 [==============================] - 2s 538ms/step - loss: 0.2822 - accuracy: 0.9370 - val_loss: 0.4700 - val_accuracy: 0.7966\n",
      "Epoch 26/30\n",
      "4/4 [==============================] - 2s 586ms/step - loss: 0.2704 - accuracy: 0.9538 - val_loss: 0.4575 - val_accuracy: 0.7797\n",
      "Epoch 27/30\n",
      "4/4 [==============================] - 2s 544ms/step - loss: 0.2584 - accuracy: 0.9454 - val_loss: 0.4732 - val_accuracy: 0.7797\n",
      "Epoch 28/30\n",
      "4/4 [==============================] - 2s 542ms/step - loss: 0.2530 - accuracy: 0.9580 - val_loss: 0.4676 - val_accuracy: 0.7966\n",
      "Epoch 29/30\n",
      "4/4 [==============================] - 2s 535ms/step - loss: 0.2300 - accuracy: 0.9664 - val_loss: 0.4631 - val_accuracy: 0.7966\n",
      "Epoch 30/30\n",
      "4/4 [==============================] - 2s 537ms/step - loss: 0.2275 - accuracy: 0.9538 - val_loss: 0.4709 - val_accuracy: 0.7966\n"
     ]
    }
   ],
   "source": [
    "## compile and evaluate\n",
    "# =========================\n",
    "\n",
    "print(\"-------Training \" + \"_\" + concat_type + \"-------\")\n",
    "batch_size = 64\n",
    "## early stop \n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "# multi_view_model.fit(train_generator, validation_data=val_generator, callbacks=[early_stopping], epochs=30,\n",
    "#                     steps_per_epoch= math.ceil(train_images_front.samples / batch_size), \n",
    "#                     validation_steps= math.ceil(val_images_front.samples / batch_size))\n",
    "## no early stop\n",
    "history = multi_view_model.fit(train_generator, validation_data=val_generator, epochs=30,\n",
    "                            steps_per_epoch= math.ceil(train_images_front.samples / batch_size), \n",
    "                            validation_steps= math.ceil(val_images_front.samples / batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\bone_20240719\\lib\\site-packages\\keras\\utils\\generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## save model to this path\n",
    "# =========================\n",
    "multi_view_model.save(\"./weights/\"+concat_type+\"_\" + \"_frac.h5\")\n",
    "# =========================\n",
    "\n",
    "\n",
    "## print results\n",
    "# =========================\n",
    "test_generator = multi_input_generator(test_images_front, test_images_side)\n",
    "\n",
    "batch_size=32\n",
    "pred = multi_view_model.predict(test_generator,  steps=math.ceil(test_images_front.samples / batch_size))\n",
    "predicted_labels = np.argmax(pred, axis=1)\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "# create plots for accuracy and save it\n",
    "# =========================\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "figAcc = plt.gcf()\n",
    "my_file = os.path.join(\"./plots/\"+concat_type+\"_Accuracy.jpeg\")\n",
    "figAcc.savefig(my_file)\n",
    "plt.clf()\n",
    "# =========================\n",
    "\n",
    "\n",
    "## create plots for loss and save it\n",
    "# =========================\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "figAcc = plt.gcf()\n",
    "my_file = os.path.join(\"./plots/\"+concat_type+\"_\"+\"_Loss.jpeg\")\n",
    "figAcc.savefig(my_file)\n",
    "plt.clf()\n",
    "# =========================\n",
    "\n",
    "\n",
    "## plot confusion matrix\n",
    "# =========================\n",
    "if class_count == 2:\n",
    "    display_labels = [0, 1]\n",
    "elif class_count == 3:\n",
    "    display_labels = [0, 1, 2]\n",
    "elif class_count == 4:\n",
    "    display_labels = [0, 1, 2, 3]\n",
    "\n",
    "\n",
    "cm = confusion_matrix(test_images_front.labels, predicted_labels)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = display_labels)\n",
    "cm_display.plot()\n",
    "plt.title('Confusion Matrix')\n",
    "figAcc = plt.gcf()\n",
    "my_file = os.path.join(\"./plots/\"+concat_type+\"_\"+\"_Confusion Matrix.jpeg\")\n",
    "figAcc.savefig(my_file)\n",
    "plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_images_front.labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def make_heatmap(img_array_list, model, pred_index=None):    \n",
    "    front_intermediate_model = tf.keras.Model(\n",
    "        inputs=model.get_layer(\"resnet50\").input,\n",
    "        outputs=model.get_layer(\"resnet50\").get_layer(\"conv5_block3_out\").output\n",
    "    )\n",
    "\n",
    "    side_intermediate_model = tf.keras.Model(\n",
    "        inputs=model.get_layer(\"efficientnetb0\").input,\n",
    "        outputs=model.get_layer(\"efficientnetb0\").get_layer(\"top_conv\").output\n",
    "    )\n",
    "\n",
    "    fornt_output = side_intermediate_model(img_array_list[0])\n",
    "    side_output = front_intermediate_model(img_array_list[1])\n",
    "\n",
    "    last_conv_layer_outputs = [fornt_output, side_output]\n",
    "    heatmaps=[]\n",
    "    for conv_output in last_conv_layer_outputs:\n",
    "        conv_output = conv_output[0]\n",
    "        heatmap = np.mean(conv_output, axis=-1)\n",
    "\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "        heatmap /= np.max(heatmap)\n",
    "        heatmaps.append(heatmap)\n",
    "    return heatmaps\n",
    "\n",
    "def save_and_display_heatmap(img_path, heatmap, alpha=0.4):\n",
    "    img = tf.keras.utils.load_img(img_path)\n",
    "    img = tf.keras.utils.img_to_array(img)\n",
    "\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    jet_heatmap = tf.keras.utils.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = tf.keras.utils.img_to_array(jet_heatmap)\n",
    "\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = tf.keras.utils.array_to_img(superimposed_img)\n",
    "    return superimposed_img\n",
    "\n",
    "def plot_heatmap_all_branches(image_path, model, save_cam):\n",
    "\n",
    "\n",
    "    # model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # 載入和處理每個圖像\n",
    "    image_paths=[image_path, image_path.replace(\"front\", \"side\")]\n",
    "    img_array1 = tf.keras.preprocessing.image.img_to_array(image.load_img(image_paths[0], target_size=(224, 224, 3)))\n",
    "    img_array2 = tf.keras.preprocessing.image.img_to_array(image.load_img(image_paths[1], target_size=(224, 224, 3)))\n",
    "    # 擴展維度以匹配模型輸入要求\n",
    "    img_array1 = np.expand_dims(img_array1, axis=0)\n",
    "    img_array2 = np.expand_dims(img_array2, axis=0)\n",
    "\n",
    "    img_arrays = [img_array1, img_array2]\n",
    "\n",
    "    heatmaps = make_heatmap(img_arrays, model)\n",
    "    # 顯示所有的熱圖\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(save_and_display_heatmap(image_paths[0], heatmaps[0]))\n",
    "    plt.title(\"AP(Mortise)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(save_and_display_heatmap(image_paths[1], heatmaps[1]))\n",
    "    plt.title(\"Lateral\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # 顯示標題\n",
    "    plt.suptitle(image_path.split('\\\\')[-1])\n",
    "    plt.savefig(save_cam)\n",
    "    plt.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_0, count=33 :\n",
      "[35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68]\n",
      "\n",
      "1_1, count=19 :\n",
      "[73, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 87, 88, 89, 91, 92, 95, 98, 99]\n",
      "\n",
      "2_2, count=28 :\n",
      "[0, 1, 3, 5, 6, 7, 9, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 24, 25, 26, 27, 29, 30, 31, 32, 33, 34]\n",
      "\n",
      "1_2, count=9 :\n",
      "[71, 72, 74, 83, 86, 90, 93, 96, 97]\n",
      "\n",
      "2_1, count=6 :\n",
      "[2, 4, 8, 14, 23, 28]\n",
      "\n",
      "0_1, count=1 :\n",
      "[39]\n",
      "\n",
      "1_0, count=3 :\n",
      "[69, 70, 94]\n",
      "\n",
      "2_0, count=1 :\n",
      "[10]\n",
      "\n",
      "0_2, count=0 :\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "def clear_directory(directory_path):\n",
    "    if os.path.exists(directory_path):\n",
    "        for filename in os.listdir(directory_path):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            if os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "            else:\n",
    "                os.remove(file_path) \n",
    "\n",
    "combinations = [(0, 0), (1, 1), (2, 2), (1, 2), (2, 1), (0, 1), (1, 0), (2, 0), (0, 2)]\n",
    "list1 = list(test_df_front['Label'].reset_index(drop=True).astype(int))\n",
    "# pred\n",
    "list2 = list(predicted_labels)\n",
    "for val1, val2 in combinations:\n",
    "    target_path = save_cam_path + str(val1) + \"_\" + str(val2)+\"//\"\n",
    "    clear_directory(target_path)\n",
    "    indices = [i for i, (v1, v2) in enumerate(zip(list1, list2)) if v1 == val1 and v2 == val2]\n",
    "    print(str(val1) + \"_\" + str(val2)+\", count=\"+str(len(indices))+\" :\")\n",
    "    print(indices)\n",
    "    print()\n",
    "\n",
    "    # 遍历匹配的索引\n",
    "    for i in indices:\n",
    "        tmp = test_df_front.reset_index(drop=True).iloc[i]['Filepath']\n",
    "        plot_heatmap_all_branches(tmp, multi_view_model, target_path+str(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen_model = \"./weights/concat1__frac.h5\"\n",
    "\n",
    "\n",
    "# for i in range(len(wrong1)):\n",
    "#     # front\n",
    "#     im_front = test_df_front.reset_index(drop=True).iloc[i]['Filepath']\n",
    "#     temp_img = image.load_img(im_front, target_size=(224, 224))\n",
    "#     x = image.img_to_array(temp_img)\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     images_front = np.vstack([x])\n",
    "#     # side\n",
    "#     im_side = test_df_front.reset_index(drop=True).iloc[i]['Filepath']\n",
    "#     temp_img = image.load_img(im_side, target_size=(224, 224))\n",
    "#     x = image.img_to_array(temp_img)\n",
    "#     x = np.expand_dims(x, axis=0)\n",
    "#     images_side = np.vstack([x])\n",
    "\n",
    "#     heatmap = make_heatmap(images_front, images_side, tf.keras.models.load_model(chosen_model))\n",
    "#     print(f\"image path={im}\")\n",
    "#     save_and_display_heatmap(im, heatmap)\n",
    "#     print(\"##################################################################################\")\n",
    "# ############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型答案寫檔\n",
    "ans_id = [\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001136744F_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002336370E_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001466426H_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002181472C_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\000113998G_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002628186H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002163835H_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\000827590J_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001132438C_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002816450D_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001264772C_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002653682D_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001466426H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002075611A_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002663741C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001247832J_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001054203H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001970344H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002294223A_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002660751C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001442921B_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001054203H_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000598291C_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000598291C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002538096F_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\000733128E_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000047083H_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\000614556I_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001334670I_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001120714A_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000047083H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002200547G_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001189670B_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001132438C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002622196F_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002549797G_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002149758F_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001189670B_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001243304E_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001292237B_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001309187H_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001154623J_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001284091E_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\000899413G_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001780977H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002358396C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001136744F_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\000127859H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000936387F_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\000034129H_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002465230A_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001264772C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002615204F_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002124062A_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002451539H_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\000833618D_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001292237B_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001960213C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001343885C_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002452272D_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002375803E_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000602012I_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001135192B_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001455017F_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\000673171B_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\000746268C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001617424G_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002561319I_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000602012I_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001455017F_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002581753C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002164833J_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\000073192A_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002309365E_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001343885C_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001683225A_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\000597192G_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002989963G_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001496597A_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000435974E_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000936387F_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002449591I_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002520589E_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000139092H_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001958124J_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000139092H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\001470230I_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002620627J_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001101501D_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\000947946D_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002838799H_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000435974E_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002812483G_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\001120714A_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002606857F_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\002658681C_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000091353B_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\正常\\\\000091353B_R.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\三踝\\\\001184477C_L.jpg',\n",
    " 'E://data_bone//4-a+b_swift_cut_正確//side\\\\雙踝\\\\002011721J_R.jpg'\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_code = []\n",
    "for i in range(len(ans_id)):\n",
    "    # print(ans_id[i])\n",
    "    matched = False\n",
    "    for j in range(len(test_df_front)):\n",
    "        filename = test_df_front['Filepath'].iloc[j].split(\"\\\\\")[-1]\n",
    "        if filename in ans_id[i]:\n",
    "            ans_code.append(predicted_labels[j])\n",
    "            matched = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for a in ans_code:\n",
    "    print(a+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bone_20240719",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
